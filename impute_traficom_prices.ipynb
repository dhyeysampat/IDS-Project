{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import setuptools\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "from tqdm.notebook import trange, tqdm\n",
    "\n",
    "load_dotenv()\n",
    "SPARK_HOST = os.getenv('SPARK_HOST')\n",
    "SPARK_APP_NAME = os.getenv('SPARK_APP_NAME')\n",
    "\n",
    "spark = SparkSession.builder.remote(SPARK_HOST).appName(SPARK_APP_NAME).getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT = os.getenv('WORKING_DIR')\n",
    "\n",
    "df = spark.read.parquet(ROOT + 'data/tax_trafi_merged_data')\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.filter(F.col('vehicle_classification') == 'M1').drop('vehicle_classification')\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rowcount = df.count()\n",
    "rowcount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "resvec = np.zeros(len(df.columns))\n",
    "\n",
    "for i, col in enumerate(df.columns):\n",
    "    try:\n",
    "        result = df.filter(F.col(col).isNull()).count()/rowcount\n",
    "    except:\n",
    "        result = None\n",
    "    resvec[i] = result\n",
    "    if result is not None:\n",
    "        print(f'percentage {col} null {result}')\n",
    "    else:\n",
    "        print(f'percentage {col} null')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nulldf = pd.DataFrame(columns = df.columns)\n",
    "nulldf.loc[0] = resvec\n",
    "nulldf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df.filter(F.col('date_of_first_registration') >= '2011-01-01')\n",
    "rowcount = df2.count()\n",
    "\n",
    "print(rowcount)\n",
    "\n",
    "resvec = np.zeros(len(df2.columns))\n",
    "\n",
    "for i, col in enumerate(df2.columns):\n",
    "    try:\n",
    "        result = df2.filter(F.col(col).isNull()).count() / rowcount\n",
    "    except:\n",
    "        result = None\n",
    "    resvec[i] = result\n",
    "    if result is not None:\n",
    "        print(f'percentage {col} null {result}')\n",
    "    else:\n",
    "        print(f'percentage {col} null')\n",
    "\n",
    "\n",
    "nulldf.loc[1] = resvec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', 0)\n",
    "nulldf.head()\n",
    "\n",
    "for i, col in enumerate(nulldf.columns):\n",
    "    val = nulldf.iloc[1, i]\n",
    "    if val < 0.01:\n",
    "        print(col, val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = df2.select('index', 'n_doors', 'n_seats', 'length_mm', 'width_mm', 'height_mm', 'drive_power', 'make_plaintext', 'model', 'transmission', 'manufac_trade_name', 'driving_power_euro_vi', 'pred_price')\n",
    "df3 = df3.drop('pred_price').na.drop(how='any').join(df3.select('index', 'pred_price'), on='index', how='left')\n",
    "df3.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_makes = df3.select('make_plaintext').distinct().collect()\n",
    "unique_makes = [r['make_plaintext'] for r in unique_makes]\n",
    "unique_makes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from tqdm.notebook import trange, tqdm\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from pyspark.sql.types import StructType, LongType, StringType, StructField, DoubleType\n",
    "\n",
    "cols = ['index', 'n_doors', 'n_seats', 'width_mm', 'height_mm', 'length_mm', 'pred_price']\n",
    "num_col = ['n_doors', 'n_seats', 'width_mm', 'height_mm', 'length_mm', 'pred_price']\n",
    "\n",
    "schema = StructType([\n",
    "    StructField('index', LongType(), True),\n",
    "    StructField('imputed_price', DoubleType(), True)\n",
    "])\n",
    "\n",
    "resultsdf = spark.createDataFrame(data = [], schema=schema)\n",
    "\n",
    "for make in tqdm(unique_makes):\n",
    "    data_subset = df3.filter(F.col('make_plaintext') == make).select(cols)\n",
    "    if data_subset.filter(F.col('pred_price').isNotNull()).count() == 0:\n",
    "        continue\n",
    "\n",
    "    data = np.array(data_subset.collect())\n",
    "    pddf = pd.DataFrame(data, columns=cols)\n",
    "    impute = pddf[num_col] # do not impute based on index\n",
    "\n",
    "    pipe = Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('imputer', KNNImputer())\n",
    "    ])\n",
    "\n",
    "    transformed = pipe.fit_transform(impute)\n",
    "    transformed = pipe.named_steps['scaler'].inverse_transform(transformed)\n",
    "    results = list(zip(pddf['index'].values, transformed[:, -1]))\n",
    "    resultsdf = resultsdf.union(spark.createDataFrame(results, schema=schema))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resultsdf.show(5)\n",
    "resultsdf.write.parquet(ROOT + 'data/knn_imputed_prices')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simplecols = pddf[num_col]\n",
    "\n",
    "# pipe2 = Pipeline([\n",
    "#     ('scaler', StandardScaler()),\n",
    "#     ('imputer', KNNImputer()),\n",
    "# ])\n",
    "\n",
    "# simpleimputeresults = pipe2.fit_transform(simplecols)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3.printSchema()\n",
    "print(df3.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from pyspark.sql.types import StructType, LongType, StringType, StructField, DoubleType\n",
    "\n",
    "\n",
    "cols = ['index', 'n_doors', 'n_seats', 'mass', 'length_mm', 'width_mm', 'height_mm', 'pred_price']\n",
    "xcol = ['n_doors', 'n_seats', 'mass', 'length_mm', 'width_mm', 'height_mm']\n",
    "ycol = ['pred_price']\n",
    "\n",
    "resultcols = ['index', 'imputed_price']\n",
    "resultsdf = pd.DataFrame()\n",
    "\n",
    "schema = StructType([\n",
    "    StructField('index', LongType(), True),\n",
    "    StructField('imputed_price', DoubleType(), True)\n",
    "])\n",
    "\n",
    "resultsdf = spark.createDataFrame(data = [], schema=schema)\n",
    "\n",
    "for make in unique_makes:\n",
    "    data = np.array(df3.filter(F.col('make_plaintext') == make).select(cols).collect())\n",
    "    pddf = pd.DataFrame(data, columns=cols)\n",
    "    pddf.head()\n",
    "    train = pddf[pddf['pred_price'].notna()]\n",
    "\n",
    "    if train.shape[0] == 0:\n",
    "        continue\n",
    "\n",
    "    train.head()\n",
    "\n",
    "    train_x = train[xcol]\n",
    "    train_y = train['pred_price']\n",
    "\n",
    "    pipe = Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('pred', LinearRegression())\n",
    "    ])\n",
    "\n",
    "    pipe.fit(train_x, train_y)\n",
    "\n",
    "    predict = pddf[~pddf['pred_price'].notna()]\n",
    "\n",
    "    if predict.shape[0] == 0:\n",
    "        continue\n",
    "\n",
    "    predict_x = predict[xcol]\n",
    "    result = pipe.predict(predict_x)\n",
    "\n",
    "    resultsdf = resultsdf.union(spark.createDataFrame(zip(predict['index'], result), schema=schema))\n",
    "\n",
    "resultsdf.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resultsdf.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resultsdf.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "resultsdf.write.parquet(ROOT + 'data/kimputed_prices')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "uni",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
